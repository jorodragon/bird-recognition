{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-19 18:10:04.820342: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import efficientnet.tfkeras as efn  # Convolutional Neural Network architecture\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_SIZE = (224, 224)\n",
    "BIRDS = [\n",
    "    \"0se\",\n",
    "    \"1hoang-yen\",\n",
    "    \"2hoa-mi\",\n",
    "    \"3tri\",\n",
    "    \"4trich-co\",\n",
    "    \"5hoet-lua\",\n",
    "    \"6le-le\",\n",
    "    \"7sao-nau\",\n",
    "    \"9chich-choe\",\n",
    "    \"8cu-dat\",\n",
    "    \"10cong-coc\",\n",
    "    \"11ca-cuong\",\n",
    "    \"12co\",\n",
    "    \"13quoc\",\n",
    "    \"14hec-xoan\",\n",
    "    \"15hoet-den\",\n",
    "    \"16phuong-hoang-dat\",\n",
    "    \"17oanh\",\n",
    "    \"18huyt-co\",\n",
    "    \"19khuou\",\n",
    "    \"20trich-re\",\n",
    "    \"21chang-nghich\",\n",
    "    \"22mo-nhat\",\n",
    "    \"23de-giun\",\n",
    "    \"24khuyen\",\n",
    "    \"25bim-bip\",\n",
    "    \"26chao-mao\",\n",
    "    \"27hut-mat\",\n",
    "    \"28cheo-beo\",\n",
    "    \"29cut\",\n",
    "    \"30sau\",\n",
    "    \"31cum-num\",\n",
    "    \"32thanh-lam\",\n",
    "    \"33bo-cau\",\n",
    "    \"34choc-quach\",\n",
    "    \"35que-lam\",\n",
    "    \"36ngu-sac\",\n",
    "    \"37hoanh-hoach\",\n",
    "    \"38en\",\n",
    "    \"39thanh-tuoc\",\n",
    "    \"40vac\",\n",
    "    \"41chia-voi\",\n",
    "    \"42mat-xeo\",\n",
    "    \"43mat-do\",\n",
    "    \"44sam-cam\",\n",
    "    \"45cu-gay\",\n",
    "    \"46ket\",\n",
    "    \"47trao-trao\",\n",
    "    \"48xanh-tim\",\n",
    "]\n",
    "DATA_PATH = \"/Volumes/MacDATA/LEARNING/thesis/code/core-ai/data/full-data/\"\n",
    "BATCH_SIZE = 16\n",
    "MODEL_PATH = \"/Volumes/MacDATA/LEARNING/thesis/source-code/birds/models/mobilenet.h5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12390 images belonging to 49 classes.\n",
      "Found 1530 images belonging to 49 classes.\n",
      "Found 1600 images belonging to 49 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "train_batches = train_datagen.flow_from_directory(\n",
    "    DATA_PATH + \"train\",\n",
    "    classes=BIRDS,\n",
    "    target_size=IM_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "valid_batches = valid_datagen.flow_from_directory(\n",
    "    DATA_PATH + \"val\",\n",
    "    classes=BIRDS,\n",
    "    target_size=IM_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_batches = test_datagen.flow_from_directory(\n",
    "    DATA_PATH + \"test\",\n",
    "    classes=BIRDS,\n",
    "    target_size=IM_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-19 18:10:22.182031: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Define CNN's architecture\n",
    "net = MobileNetV2(include_top=False,\n",
    "                  weights='imagenet',\n",
    "                  input_tensor=None,\n",
    "                  input_shape=(224, 224, 3))\n",
    "x = net.output\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(len(BIRDS), activation=\"softmax\", name=\"softmax\")(x)\n",
    "net_final = Model(inputs=net.input, outputs=output_layer)\n",
    "net_final.compile(\n",
    "    optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# print(net_final.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate class weights for unbalanced dataset\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(train_batches.classes), y=train_batches.classes\n",
    ")\n",
    "\n",
    "class_weights = {i:w for i,w in enumerate(class_weights)}\n",
    "\n",
    "# Define callbacks\n",
    "ModelCheck = ModelCheckpoint(\n",
    "    MODEL_PATH,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode=\"auto\",\n",
    ")\n",
    "\n",
    "ReduceLR = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=5, min_lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 88/775 [==>...........................] - ETA: 34:26 - loss: 6.7875 - accuracy: 0.0973"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "net_final.fit(\n",
    "    train_batches,\n",
    "    validation_data=valid_batches,\n",
    "    epochs=30,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[ModelCheck, ReduceLR],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************\n",
      "Class nr  0  ->  0se\n",
      "Class nr  1  ->  1hoang-yen\n",
      "Class nr  2  ->  2hoa-mi\n",
      "Class nr  3  ->  3tri\n",
      "Class nr  4  ->  4trich-co\n",
      "Class nr  5  ->  5hoet-lua\n",
      "Class nr  6  ->  6le-le\n",
      "Class nr  7  ->  7sao-nau\n",
      "Class nr  8  ->  9chich-choe\n",
      "Class nr  9  ->  8cu-dat\n",
      "Class nr  10  ->  10cong-coc\n",
      "Class nr  11  ->  11ca-cuong\n",
      "Class nr  12  ->  12co\n",
      "Class nr  13  ->  13quoc\n",
      "Class nr  14  ->  14hec-xoan\n",
      "Class nr  15  ->  15hoet-den\n",
      "Class nr  16  ->  16phuong-hoang-dat\n",
      "Class nr  17  ->  17oanh\n",
      "Class nr  18  ->  18huyt-co\n",
      "Class nr  19  ->  19khuou\n",
      "Class nr  20  ->  20trich-re\n",
      "Class nr  21  ->  21chang-nghich\n",
      "Class nr  22  ->  22mo-nhat\n",
      "Class nr  23  ->  23de-giun\n",
      "Class nr  24  ->  24khuyen\n",
      "Class nr  25  ->  25bim-bip\n",
      "Class nr  26  ->  26chao-mao\n",
      "Class nr  27  ->  27hut-mat\n",
      "Class nr  28  ->  28cheo-beo\n",
      "Class nr  29  ->  29cut\n",
      "Class nr  30  ->  30sau\n",
      "Class nr  31  ->  31cum-num\n",
      "Class nr  32  ->  32thanh-lam\n",
      "Class nr  33  ->  33bo-cau\n",
      "Class nr  34  ->  34choc-quach\n",
      "Class nr  35  ->  35que-lam\n",
      "Class nr  36  ->  36ngu-sac\n",
      "Class nr  37  ->  37hoanh-hoach\n",
      "Class nr  38  ->  38en\n",
      "Class nr  39  ->  39thanh-tuoc\n",
      "Class nr  40  ->  40vac\n",
      "Class nr  41  ->  41chia-voi\n",
      "Class nr  42  ->  42mat-xeo\n",
      "Class nr  43  ->  43mat-do\n",
      "Class nr  44  ->  44sam-cam\n",
      "Class nr  45  ->  45cu-gay\n",
      "Class nr  46  ->  46ket\n",
      "Class nr  47  ->  47trao-trao\n",
      "Class nr  48  ->  48xanh-tim\n",
      "****************\n",
      "accuracy: 89.44%\n",
      "100/100 [==============================] - 25s 235ms/step\n",
      "Confusion Matrix\n",
      "[[84  0  0 ...  0  0  0]\n",
      " [ 0 15  0 ...  0  0  0]\n",
      " [ 0  0 41 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 11  0  0]\n",
      " [ 0  0  0 ...  0 43  0]\n",
      " [ 0  0  0 ...  0  0 18]]\n",
      "Classification Report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "               0se       1.00      1.00      1.00        84\n",
      "        1hoang-yen       0.52      0.83      0.64        18\n",
      "           2hoa-mi       0.89      0.75      0.81        55\n",
      "              3tri       0.57      1.00      0.72        13\n",
      "         4trich-co       0.68      0.94      0.79        18\n",
      "         5hoet-lua       0.68      0.79      0.73        19\n",
      "            6le-le       1.00      1.00      1.00        16\n",
      "          7sao-nau       0.79      0.82      0.81        28\n",
      "       9chich-choe       0.94      0.60      0.74        81\n",
      "           8cu-dat       1.00      0.96      0.98        28\n",
      "        10cong-coc       1.00      1.00      1.00        11\n",
      "        11ca-cuong       1.00      1.00      1.00        10\n",
      "              12co       0.93      0.50      0.65        26\n",
      "            13quoc       0.95      1.00      0.97        52\n",
      "        14hec-xoan       0.95      1.00      0.97        18\n",
      "        15hoet-den       0.58      1.00      0.74        28\n",
      "16phuong-hoang-dat       0.76      1.00      0.86        16\n",
      "            17oanh       1.00      1.00      1.00        19\n",
      "         18huyt-co       0.69      1.00      0.82        18\n",
      "           19khuou       0.81      0.94      0.87        95\n",
      "        20trich-re       0.98      0.96      0.97        55\n",
      "    21chang-nghich       1.00      1.00      1.00        12\n",
      "         22mo-nhat       1.00      1.00      1.00        19\n",
      "         23de-giun       0.95      0.98      0.96        54\n",
      "          24khuyen       0.72      0.90      0.80        52\n",
      "         25bim-bip       0.97      0.85      0.91        68\n",
      "        26chao-mao       0.93      0.95      0.94       133\n",
      "         27hut-mat       1.00      0.85      0.92        26\n",
      "        28cheo-beo       1.00      1.00      1.00        25\n",
      "             29cut       1.00      0.96      0.98        74\n",
      "             30sau       1.00      1.00      1.00        45\n",
      "         31cum-num       1.00      1.00      1.00        14\n",
      "       32thanh-lam       1.00      1.00      1.00        15\n",
      "          33bo-cau       1.00      1.00      1.00        11\n",
      "      34choc-quach       0.61      1.00      0.76        11\n",
      "         35que-lam       0.85      1.00      0.92        11\n",
      "         36ngu-sac       1.00      1.00      1.00        13\n",
      "     37hoanh-hoach       0.90      0.64      0.75        14\n",
      "              38en       0.85      0.30      0.44        74\n",
      "      39thanh-tuoc       0.95      0.95      0.95        19\n",
      "             40vac       0.90      1.00      0.95        18\n",
      "        41chia-voi       1.00      1.00      1.00         9\n",
      "         42mat-xeo       1.00      1.00      1.00        18\n",
      "          43mat-do       1.00      1.00      1.00        18\n",
      "         44sam-cam       1.00      1.00      1.00        18\n",
      "          45cu-gay       0.94      1.00      0.97        49\n",
      "             46ket       1.00      1.00      1.00        11\n",
      "       47trao-trao       1.00      1.00      1.00        43\n",
      "        48xanh-tim       0.86      1.00      0.92        18\n",
      "\n",
      "          accuracy                           0.89      1600\n",
      "         macro avg       0.90      0.93      0.90      1600\n",
      "      weighted avg       0.91      0.89      0.89      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show class indices\n",
    "print('****************')\n",
    "for cls, idx in test_batches.class_indices.items():\n",
    "    print('Class nr ',idx,' -> ', cls)\n",
    "print('****************')\n",
    "\n",
    "loaded_model = Model(inputs=net.input, outputs=output_layer)\n",
    "\n",
    "loaded_model.load_weights(MODEL_PATH)\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(optimizer=Adam(learning_rate=5e-5),\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                  \n",
    "score = loaded_model.evaluate(test_batches, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "\n",
    "Y_pred = loaded_model.predict(test_batches)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_batches.classes, y_pred))\n",
    "print('Classification Report')\n",
    "print(classification_report(test_batches.classes, y_pred, target_names=BIRDS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
